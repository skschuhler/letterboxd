{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I haven't scraped film data (directors, writers, genres, etc.) yet, I'll start with a collaborative filtering approach. Ultimately, I plan to compare these results to a content-based methodology, and then potentially ensemble the two systems. \n",
    "\n",
    "There are two key types of collaborative filtering approaches: memory-based and model-based. Memory-based approaches (user-based and item-based) hinge on grouping similar users/items, and using those similarities to inform the recommendations. For example, if Rob and Josie like a lot of the same movies, then Rob will be recommended a movie that Josie enjoyed. I decided to start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sparse\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, mean_squared_error, f1_score, roc_auc_score, confusion_matrix\n",
    "from implicit import als\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/combined_data.csv')\n",
    "\n",
    "# Create outcome variable\n",
    "df['enjoyed'] = df.apply(lambda row: 1 if row['Rating'] >= 4 or row['Liked'] == 1 else 0, axis=1)\n",
    "\n",
    "user_item_matrix = df.pivot_table(index='user', columns='Title', values='enjoyed', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=4)\n",
    "\n",
    "user_item_sparse_matrix = csr_matrix(user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guess Accuracy: 0.5006856286371316\n",
      "Random Guess Precision: 0.4220181735441347\n",
      "Random Guess Recall: 0.5002514455946557\n",
      "Random Guess F1-Score: 0.45781666662685416\n",
      "Random Guess AUC: 0.5006266511465138\n",
      "Confusion Matrix (Random Guess): \n",
      "           Predicted: 0  Predicted: 1\n",
      "Actual: 0        395308        393727\n",
      "Actual: 1        287194        287483\n"
     ]
    }
   ],
   "source": [
    "# Random Guess Baseline\n",
    "random_predictions = np.random.choice([0, 1], size=len(test_data))\n",
    "\n",
    "# Accuracy for random baseline\n",
    "accuracy_random = accuracy_score(test_data['enjoyed'], random_predictions)\n",
    "print(f\"Random Guess Accuracy: {accuracy_random}\")\n",
    "\n",
    "# Precision, Recall, and F1 for Random Guess Baseline\n",
    "precision_random = precision_score(test_data['enjoyed'], random_predictions)\n",
    "recall_random = recall_score(test_data['enjoyed'], random_predictions)\n",
    "f1_random = f1_score(test_data['enjoyed'], random_predictions)\n",
    "\n",
    "print(f\"Random Guess Precision: {precision_random}\")\n",
    "print(f\"Random Guess Recall: {recall_random}\")\n",
    "print(f\"Random Guess F1-Score: {f1_random}\")\n",
    "\n",
    "# ROC AUC for random baseline\n",
    "auc_random = roc_auc_score(test_data['enjoyed'], random_predictions)\n",
    "print(f\"Random Guess AUC: {auc_random}\")\n",
    "\n",
    "# Confusion Matrix for Random Guess Baseline\n",
    "conf_matrix_random = confusion_matrix(test_data['enjoyed'], random_predictions)\n",
    "# Convert the confusion matrix to a pandas DataFrame for better readability\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix_random, \n",
    "                               columns=['Predicted: 0', 'Predicted: 1'], \n",
    "                               index=['Actual: 0', 'Actual: 1'])\n",
    "print(f\"Confusion Matrix (Random Guess): \\n{conf_matrix_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user similarity matrix using cosine similarity\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Convert the user similarity matrix into a DataFrame for better readability\n",
    "user_similarity_df = pd.DataFrame(user_similarity, \n",
    "                                  index=user_item_matrix.index, \n",
    "                                  columns=user_item_matrix.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to check if a key exists in the data\n",
    "def check_data_availability(user_id, movie_title, user_similarity_df, user_item_matrix):\n",
    "    # Check if user_id exists in the similarity dataframe\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        print(f\"Warning: {user_id} not found in user_similarity_df\")\n",
    "        return False\n",
    "    # Check if movie_title exists in the user-item matrix\n",
    "    if movie_title not in user_item_matrix.columns:\n",
    "        print(f\"Warning: {movie_title} not found in user_item_matrix\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def predict_user_based_binary(user_id, movie_title, user_similarity_df, user_item_matrix, top_n=5):\n",
    "    # Check if the user and movie are available in the data\n",
    "    if not check_data_availability(user_id, movie_title, user_similarity_df, user_item_matrix):\n",
    "        return 0  # Fallback to predicting 0 if the user or movie is missing\n",
    "    # Get the list of users who have rated the movie\n",
    "    movie_ratings = user_item_matrix[movie_title]\n",
    "    # Get the similarity scores for the target user with all other users\n",
    "    similarities = user_similarity_df[user_id]\n",
    "    # Filter out the target user from the similarity scores (no need to compare with themselves)\n",
    "    similarities = similarities.drop(user_id)\n",
    "    # Get the top N most similar users\n",
    "    top_similar_users = similarities.nlargest(top_n).index\n",
    "    # Get ratings of the top N similar users for the movie\n",
    "    top_ratings = movie_ratings[top_similar_users]\n",
    "    # Ensure we don't divide by zero: Check if the sum of similarities is non-zero\n",
    "    similarity_sum = np.sum(similarities[top_similar_users])\n",
    "    # Calculate the weighted average of ratings based on the similarity scores\n",
    "    if similarity_sum == 0:\n",
    "        # If no similarity (i.e., sum is zero), we can use a fallback approach.\n",
    "        # Option 1: Use global average rating (or user average rating)\n",
    "        weighted_avg = user_item_matrix.loc[user_id].mean()  # For user-based fallback\n",
    "    else:\n",
    "        # Calculate the weighted average of ratings based on the similarity scores\n",
    "        weighted_avg = np.dot(similarities[top_similar_users], top_ratings) / similarity_sum\n",
    "    # Convert the weighted average to a binary prediction (1 if weighted_avg > 0.5, else 0)\n",
    "    return 1 if weighted_avg > 0.5 else 0\n",
    "\n",
    "predict_user_based_binary(\"noir1946\", \"The Ox-Bow Incident\", user_similarity_df, user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b90c76ce154b969c3c204a23a77a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|                                                       | 0/1363712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Warning: nan not found in user_item_matrix\n",
      "Accuracy: 0.6785750950347288\n",
      "Precision: 0.6970135709992139\n",
      "Recall: 0.41969488947704536\n",
      "F1-Score: 0.5239201528387363\n",
      "AUC-ROC: 0.6434099609767124\n",
      "Confusion Matrix:\n",
      "[[684192 104843]\n",
      " [333488 241189]]\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate using multiple metrics\n",
    "def evaluate_user_based_metrics(user_item_matrix, user_similarity_df, test_data):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    # Wrap the iteration with tqdm to show a progress bar\n",
    "    for _, row in tqdm(test_data.iterrows(), total=test_data.shape[0], desc=\"Evaluating\", ncols=100):\n",
    "        user_id = row['user']\n",
    "        movie_title = row['Title']\n",
    "        actual_rating = row['enjoyed']\n",
    "        \n",
    "        # Predict the rating for the user-movie pair (binary)\n",
    "        predicted_rating = predict_user_based_binary(user_id, movie_title, user_similarity_df, user_item_matrix)\n",
    "        \n",
    "        predictions.append(predicted_rating)\n",
    "        actuals.append(actual_rating)\n",
    "    \n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    precision = precision_score(actuals, predictions)\n",
    "    recall = recall_score(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions)\n",
    "    auc_roc = roc_auc_score(actuals, predictions)\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(f\"AUC-ROC: {auc_roc}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Assuming you have a test dataset (test_data)\n",
    "evaluate_user_based_metrics(user_item_matrix, user_similarity_df, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: 0.6785750950347288\n",
    "- Precision: 0.6970135709992139\n",
    "- Recall: 0.41969488947704536\n",
    "- F1-Score: 0.5239201528387363\n",
    "- AUC-ROC: 0.6434099609767124\n",
    "- Confusion Matrix:\n",
    "- [[684192 104843]\n",
    " [333488 241189]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lb_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
